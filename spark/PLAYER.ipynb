{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players =  spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .load(\"file:///home/icpl12900/Desktop/assignments/Spark_data/player.csv\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_attribute = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\",\"true\")\\\n",
    "                .load(\"file:///home/icpl12900/Desktop/assignments/Spark_data/player_attributes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "player_attribute.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year_extract_udf = udf(lambda date : date.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_player_attribute = player_attribute.withColumn('year' , year_extract_udf(player_attribute.date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_player_attribute.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016 = new_player_attribute.filter(new_player_attribute.year == 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_striker_2016 = pa_2016.groupBy('player_api_id').\\\n",
    "                        agg({\"finishing\" : 'avg',\n",
    "                            'shot_power':'avg',\n",
    "                            'acceleration':'avg'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_striker_2016.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_striker_2016=pa_striker_2016.withColumnRenamed('avg(finishing)','finishing')\\\n",
    "              .withColumnRenamed('avg(acceleration)','acceleration')\\\n",
    "              .withColumnRenamed('avg(shot_power)','shot_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_striker_2016.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finishing = 1\n",
    "acceleration = 2\n",
    "shot_power = 1\n",
    "total = finishing + acceleration + shot_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker = pa_striker_2016.withColumn(\"striker_grade\" , (pa_striker_2016.finishing * finishing +\\\n",
    "                                                       pa_striker_2016.acceleration * acceleration +\\\n",
    "                                                       pa_striker_2016.shot_power * shot_power)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker = striker.sort(striker.striker_grade.desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker=striker.drop('finishing','acceleration','shot_power')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_striker = striker.join(players,players.player_api_id == striker.player_api_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_striker.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_striker = striker.join(players,['player_api_id'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_striker.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BROADCAST VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WHEN DATASETS IS HUGE THEN JOIN OPERATIONS ON THAT DATASET IS VERY HEAVY OPERATION. BCOZ DATASETS ARE DISTRIBUTE ACROSS NODES IN CLUSTER. SO USE BROADCAST IN WHICH ONE DATASET WHICH IS SMALL IS COPY TO WORKER NODE AND JOIN PRFORM OVER THERE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker_details = players.select('player_api_id' ,'player_name').\\\n",
    "                            join(\n",
    "                                    broadcast(striker),\n",
    "                                    ['player_api_id'],\n",
    "                                    'inner' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "striker_details.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACCUMULATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIGH HEADING ACCURACY DIVIDED BY BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_heading_acc = player_attribute.select('player_api_id','heading_accuracy').\\\n",
    "                                       join(\n",
    "                                            broadcast(players),\n",
    "                                                 ['player_api_id'],\n",
    "                                                'inner' \n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_heading_acc.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HEIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short = spark.sparkContext.accumulator(0)\n",
    "medium = spark.sparkContext.accumulator(0)\n",
    "tall = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_player_height(row):\n",
    "    height = float(row.height)\n",
    "    if (height<=175):\n",
    "        short.add(1)\n",
    "    elif (height <= 195):\n",
    "        medium.add(1)\n",
    "    else:\n",
    "        tall.add(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_heading_acc.foreach(lambda x : count_player_height(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_players = [short.value,\n",
    "              medium.value,\n",
    "              tall.value]\n",
    "all_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heading_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_ha = spark.sparkContext.accumulator(0)\n",
    "medium_ha = spark.sparkContext.accumulator(0)\n",
    "tall_ha = spark.sparkContext.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_player_headingAccuracy(row):\n",
    "    head = row.heading_accuracy\n",
    "    if(head>=75 and head<=175):\n",
    "        short_ha.add(1)\n",
    "    elif (head <= 195):\n",
    "        medium_ha.add(1)\n",
    "    else:\n",
    "        tall_ha.add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "players_heading_acc.foreach(lambda x : count_player_headingAccuracy(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_players_heading = [short_ha.value,\n",
    "                      medium_ha.value,\n",
    "                      tall_ha.value]\n",
    "all_players_heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "percentage_value = [(short_ha.value/short.value) *100,\n",
    "                      (medium_ha.value/medium.value) *100,\n",
    "                      (tall_ha.value/tall.value) *100]\n",
    "\n",
    "percentage_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORE DATAFRAME TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016.select( 'player_api_id','date')\\\n",
    "        .coalesce(1)\\\n",
    "        .write\\\n",
    "        .option(\"header\",\"true\")\\\n",
    "        .csv(\"file:///home/icpl12900/Desktop/assignments/Spark_data/player_date.csv\")\n",
    "        \n",
    "#coalesce(1) --- to repartitioned DataFrame to single partition, to write into single file. here we want \n",
    "#one partition  so use 1 as argument\n",
    "#player_date.csv it is directiory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE CUSTOM ACCUMULATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VectorAccumulator(AccumulatorParam):\n",
    "    def zero(self,value):\n",
    "        return [0.0] * len(value)\n",
    "    def addInPlace(self,v1,v2):\n",
    "        for i in range(len(v1)):\n",
    "            v1[i] += v2[i]\n",
    "        return v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_acc = sc.accumulator([10.0,20.0],VectorAccumulator())\n",
    "vector_acc.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASIC SQL OPERATIONS ON SPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016.createOrReplaceTempView('table_2016')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It create temporary view for dataFrame pa_2016 and it is temporary . It create only for this session. Once session is closed view also disappear\n",
    "#### It Register DataFrame as Table For Per-Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = sqlContext.sql(\"select * from table_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql('select id from table_2016 where id == 1').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To make avaliable Global table to all SPARK SESSION  use \"createGlobalTempView\"\n",
    "#### And to access this global table use  \"global_temp.tableName\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pa_2016.createGlobalTempView('table_2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.sql(\"select * from global_temp.table_2016\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
